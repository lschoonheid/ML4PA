{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Dimensional Reduction and Clustering\n",
    "BSc ML in Physics & Astronomy course, Sep 2021\n",
    "\n",
    "Author: Ryan van Mastrigt\n",
    "\n",
    "In this tutorial we will use dimensional reduction techniques, specifically PCA and t-SNE, to reduce high dimensional data to two dimensions such that the data can be easily visualized. Next, clustering techniques can be used on the reduced data to find ordering in the data. In this tutorial we will work with Monte Carlo-generated spin configurations for the two-dimensional Ising model on a square lattice over a range of temperatures. This model is known to have a phase transition from an ordered to a disorded state at $T_c = \\frac{2 J}{k \\ln{(1 + \\sqrt{2})}}$. In the data we use $J = k = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "First things first, we need to load in the packages we will use throughout this tutorial. Additionally we will load in the Ising data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "data = np.load(u'.\\\\data\\\\MC_Ising_2Dsquare_10x10.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - PCA on the Ising model\n",
    "Note that the loaded data has a .npz file format; this is Numpy's file format for a zipped archive of files. First we will explore the data a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To see the files in the zipped archive, use .files extention\n",
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = data['configs']\n",
    "T = data['temperatures']\n",
    "\n",
    "#For every unique temperature, plot a random image from the configs\n",
    "plt.figure(figsize=(20, 10))\n",
    "for i in range(np.shape(T)[0]):\n",
    "    j = np.random.randint(0, np.shape(configs)[1])\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    plt.imshow(configs[i, j], cmap='Greys', vmin=-1, vmax=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A) The order parameter\n",
    "Do you see what changes in the configurations when the temperature is increased? Can you think of a parameter that would capture these changes in a single value? Does this parameter have any physical meaning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical quantities of the Ising model\n",
    "The Ising model in its most simple form is described by the Hamiltonian $H=-J \\sum_{(i, j)} s_i s_j$, where $s_i \\in \\{-1, +1\\}$ is the spin on site $i$, and $(i, j)$ indicates a sum over nearest neighbours. From the configurational data, we are able to define multiple physical quantities of the system as a function of temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_magnetization_and_susceptibility(configs):\n",
    "    # the magnetization per configuration is given by the sum of all spins, here we take the absolute magnetization\n",
    "    # sum over each configuration and normalize\n",
    "    Mag = np.sum(configs, axis=(2, 3))\n",
    "    Mag = np.divide(Mag, np.shape(configs)[2] * np.shape(configs)[3])\n",
    "    AbsMag = np.abs(Mag)\n",
    "    MagSq = np.square(AbsMag)\n",
    "    #sum over configs per temperature\n",
    "    Mag = np.sum(Mag, axis=1) / np.shape(configs)[1]\n",
    "    AbsMag = np.sum(AbsMag, axis=1) / np.shape(configs)[1]\n",
    "    MagSq = np.sum(MagSq, axis=1) / np.shape(configs)[1]\n",
    "    Chi = np.divide(np.add(MagSq, -np.square(AbsMag)), T)\n",
    "    return Mag, AbsMag, Chi\n",
    "\n",
    "def calc_energy(configs):\n",
    "    # the energy per configuration can be determined from the Hamiltonian\n",
    "    Energy = np.zeros(np.shape(configs)[0])\n",
    "    for t in range(np.shape(configs)[0]):\n",
    "        En = 0\n",
    "        for c in range(np.shape(configs)[1]):\n",
    "            # For each configuration, calculate the energy by multiplying each site with its \n",
    "            # horizontal and vertical nearest neighbour and summing the total\n",
    "            En += np.sum(np.multiply(-configs[t, c], np.roll(configs[t, c], 1, axis=0)))\n",
    "            En += np.sum(np.multiply(-configs[t, c], np.roll(configs[t, c], 1, axis=1)))\n",
    "        En /= 2 * np.shape(configs)[1] * np.shape(configs)[2] * np.shape(configs)[3]\n",
    "        Energy[t] = En\n",
    "    return Energy\n",
    "\n",
    "Mag, AbsMag, Chi = calc_magnetization_and_susceptibility(configs)\n",
    "Energy = calc_energy(configs)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(T, Energy, '.-')\n",
    "plt.xlabel('$T$')\n",
    "plt.ylabel('$<E>$')\n",
    "plt.title('Average energy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(T, Mag, '.-')\n",
    "plt.xlabel('$T$')\n",
    "plt.ylabel('$<M>$')\n",
    "plt.title('Average magnetization')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(T, AbsMag, '.-')\n",
    "plt.xlabel('$T$')\n",
    "plt.ylabel('$<|M|>$')\n",
    "plt.title('Average absolute magnetization')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(T, Chi, '.-')\n",
    "plt.xlabel('$T$')\n",
    "plt.ylabel('$|\\chi|$')\n",
    "plt.title('Magnetic susceptibility')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B) The magnetization\n",
    "The average magnetization $\\langle M \\rangle$ seems to switch from -1 to 1 at low temperatures. Can you give an explenation of why this might be happening? Do you think this comes from the numeric technique used to generate the data (Metropolis algorithm) or does it portray physical behaviour inherent to the model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "If we would want to display each configuration as a datapoint in phase space, we would need a $10 \\times 10 = 100$ dimensional image. Unsurprisingly, this is quite difficult to visualise. Dimensional reduction techniques can come in handy to help visualise your data and find trends. Onse such simple and effective dimensional reduction technique is Principle Component Analysis, or PCA. In what follows we will derive PCA in a general case, after which you will write code for applying it to the Ising model data.\n",
    "\n",
    "Suppose we have a $n \\times p$ data matrix $X$, where each of the $n$ rows represents a unique measurement of all the $p$ features. For what follows, we assume the mean of each column has been shifted to zero. We are then looking for a set of a given size, say $l$, of $p$-dimensional vectors $w_{(k)} = (w_1, ..., w_p)_{(k)}$ that map each row vector $x_{(i)}$ of $X$ to a new vector of principal component scores $t_{(i)} = (t_1, ..., t_l)_{(i)}$, which are calculated as\n",
    "\\begin{equation}\n",
    "    t_{k(i)} = x_{(i)} \\cdot w_{(k)}, \\quad \\mathrm{for} \\quad i=1, ..., n \\quad k=1, ..., l.\n",
    "\\end{equation}\n",
    "Such that the components of $t$ successively have the maximum possible variance from $X$, and where $w$ is constrained to be a unit vector.\n",
    "\n",
    "For our data matrix, we can then define the empirical covariance matrix as\n",
    "\\begin{equation}\n",
    "    Q = X^T X.\n",
    "\\end{equation}\n",
    "Since we want to find the largest value for the first component of $t$, this equates to finding\n",
    "\\begin{equation}\n",
    "    w_{(1)} = \\mathrm{arg max} \\sum_i (t_1)^2_{(i)} = \\mathrm{arg max}\\frac{w^T Q w}{w^T w},\n",
    "\\end{equation}\n",
    "which can be recognised as the Rayleigh quotient. For a positive semidefinite matrix like $Q$ the quotient's maximum possible value is the largest eigenvalue of the matrix, so $w$ needs to be the corresponding eigenvector.\n",
    "\n",
    "To find the other principal components, note that the covariance matrix can be written as\n",
    "\\begin{equation}\n",
    "    Q = W \\Lambda W^T,\n",
    "\\end{equation}\n",
    "where W is a $p \\times p$ matrix whose columns are the eigenvectors of $Q$, and $\\Lambda$ is the diagonal matrix of eigenvalues $\\lambda_{(k)}$ of $Q$.\n",
    "The covariance matrix between the principal components then becomes\n",
    "\\begin{equation}\n",
    "    W^T Q W = W^T W \\Lambda W^T W = \\Lambda.\n",
    "\\end{equation}\n",
    "$\\lambda_{(k)}$ is equal to the sum of the squares over the dataset associated with each component $k$: \n",
    "\\begin{equation}\n",
    "    \\lambda_{(k)} = \\sum_i (t_k^2)_{(i)} = \\sum_i (x_{(i)} \\cdot w_{(k)})^2.\n",
    "\\end{equation}\n",
    "In summary, the principal components are eigenvectors of the data's covariance matrix, and the top principal components correspond to the directions in the data-space that preserve as much of the data's variation as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task C) Set up PCA on the Ising data\n",
    "Use numpy to set up PCA on the Ising data, and plot the data onto the first two principle components. How much variance can be explained with the first two components? Can you see to which physical quantity the first principle component relates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First transform the data to a n by p matrix\n",
    "configs_PCA = np.reshape(configs, (-1, np.shape(configs)[2] * np.shape(configs)[3]))\n",
    "\n",
    "# Shift the data such that the mean of each column has been shifted to zero \n",
    "# and normalize such that they have the same standard deviation\n",
    "## enter code ##\n",
    "\n",
    "# calculate the covariance matrix (tip: use numpy's built in functions)\n",
    "## enter code ##\n",
    "\n",
    "# Find the eigendecomposition of the covariance matrix\n",
    "## enter code ##\n",
    "\n",
    "# Visualize the eigenvalues from high to low\n",
    "## enter code ##\n",
    "\n",
    "# Project the data onto the first two principal components and plot the results \n",
    "# (tip: try colouring each configuration with a unique colour per temperature)\n",
    "## enter code ##\n",
    "\n",
    "# Take the average per temperature of the absolute value over the projected data for the first two components\n",
    "# and plot as a function of temperature. Which physical quantities do the components resemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task D) Plot the Principal components\n",
    "One of the advantages of a linear technique like PCA, is that it is interpretable. The principle components have a clear relationship to the original data. It can be useful to plot the components to see what exactly they do. Since the original data is two-dimensional, it makes sense to transform the principal components to 2D as well to see how they work on each part of the lattice. Please plot the two principle components. Can you see what the second principle component looks at? Tip: Fourier transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the principal components to two dimensions\n",
    "# eigen_vectors are the principle components, and is of shape (p, l)\n",
    "eigen_vectors = \n",
    "PC_transformed = np.reshape(eigen_vectors, (np.shape(configs)[2], np.shape(configs)[3], np.shape(eigen_vectors)[1]))\n",
    "\n",
    "# plot the results\n",
    "vmin = np.amin(eigen_vectors[:, 0:2])\n",
    "vmax = np.amax(eigen_vectors[:, 0:2])\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(PC_transformed[:, :, 0], vmin=vmin, vmax=vmax)\n",
    "plt.title('PC1')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.roll(PC_transformed[:, :, 1], 6), vmin=vmin, vmax=vmax)\n",
    "plt.title('PC2')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - K-means clustering\n",
    "Next to dimensional reduction techniques, clustering techniques can be useful for identifying similar datapoints and grouping them accordingly. In what follows, you will apply K-means clustering to the PCA-projected Ising data after a brief theoretical introduction to the technique.\n",
    "\n",
    "If we again assume we have a $n \\times p$ data set  $X$, k-means clustering tries to partition the $n$ observations into $k$ (to be defined by the user) sets $S = \\{S_1, S_2, ..., S_k\\}$ such that the within-cluster variance is minimized. More formally this can be expressed as:\n",
    "\\begin{equation}\n",
    "    \\mathrm{arg min}_S \\sum_{i=1}^{k} \\sum_{x \\in S_i} ||x - \\mu_i||^2 = \\mathrm{arg min}_S \\sum_{i=1}^{k} |S_i| \\mathrm{Var}S_i,\n",
    "\\end{equation}\n",
    "where $\\mu_i$ is the mean of all points in $S_i$.\n",
    "\n",
    "In its most naive form, the solution to this problem can be approximated through an iterative algorithm. \n",
    "\n",
    "Given a set of means $m_1^{(1)}, ..., m_k^{(1)}$ the algorithm first assigns each observation to the cluster with the nearest (in Euclidian distance) mean. The means are then updated as\n",
    "\\begin{equation}\n",
    "    m_i^{(t+1)} = \\frac{1}{|S_i^{(t)}|} \\sum_{x_j \\in S_i^{(t)}} x_j.\n",
    "\\end{equation}\n",
    "This process repeats itself until the means converge. However, this algorithm is not guaranteed to find the optimum.\n",
    "\n",
    "There are multiple ways of chosing the initial means, that can have a large effect on the effectiveness of the method. Here we will use the so-called k-means++ approach. The algorithm goes as follows:\n",
    "1. Choose the first cluster center uniformly at random from all the data points.\n",
    "2. For each data point $x$ not chosen yet, compute D(x): the distance between x and the nearest center.\n",
    "3. Chose one new data point at random as a new center, using a weighted probability distribution where point $x$ is weighted proportional to $D(x)^2$.\n",
    "4. Repeat steps 2 and 3 until $k$ centers have been chosen.\n",
    "5. Proceed to normal k-means clustering.\n",
    "\n",
    "For this assignment, please finish the code below to complete the k-means algorithm. \n",
    "1. Show the sum of squared error as a function of number of iterations. \n",
    "2. How fast do you converge to a solution? \n",
    "3. Does this change if you initialize your clusters at random instead of with k-means++?\n",
    "4. Plot each cluster with a different colour in a scatter plot. Does the clustering make sense to you? \n",
    "5. What is a natural number of clusters for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "# initialize your data (in this case, first two projected PCA components)\n",
    "# Proj is the Ising data projected along the principle components, of shape (n, l)\n",
    "Proj = \n",
    "kdat = Proj[:, 0:2]\n",
    "# set k\n",
    "k = \n",
    "\n",
    "# array to store centroids\n",
    "centroids = np.zeros((k, np.shape(kdat)[1]))\n",
    "\n",
    "# k-means++ initialization\n",
    "# array to store distance each datapoint to each centroid\n",
    "dist = np.zeros((k, np.shape(kdat)[0]))\n",
    "\n",
    "# sample initial centroids\n",
    "random_index = np.random.randint(np.shape(kdat)[0])\n",
    "centroids[0] = kdat[random_index]\n",
    "d = np.add(kdat, -centroids[0])\n",
    "d = np.square(d)\n",
    "d = np.sum(d, axis=1)\n",
    "dist[0] = d\n",
    "prob = dist[0] / np.sum(dist[0])\n",
    "random_index = np.random.choice(np.shape(kdat)[0], p = prob)\n",
    "for i in range(1, k):\n",
    "    centroids[i] = kdat[random_index]\n",
    "    d = np.add(kdat, -centroids[i])\n",
    "    d = np.square(d)\n",
    "    d = np.sum(d, axis=1)\n",
    "    dist[i] = d\n",
    "    prob = np.amin(dist[:i+1], axis=0)\n",
    "    prob /= np.sum(prob)\n",
    "    random_index = np.random.choice(np.shape(kdat)[0], p = prob)\n",
    "\n",
    "# list of the sum of squared estimated of errors to keep track during k-means\n",
    "sse_list = []\n",
    "\n",
    "# function to compute the L2 distance between data x of form [n, p] and centroid of form [p]\n",
    "def compute_l2_distance(x, centroid):\n",
    "    dist = ((x - centroid) ** 2).sum(axis = 1)\n",
    "    return dist\n",
    "\n",
    "# function that finds the closest centroid for data x of form [n, p] and set of centroids of form [k, p]\n",
    "def get_closest_centroid(x, centroids):\n",
    "    dist = []\n",
    "    for c in range(len(centroids)):\n",
    "        dist.append(compute_l2_distance(x, centroids[c]))\n",
    "    closest_centroid_index = np.argmin(np.array(dist), axis=0)\n",
    "    return closest_centroid_index\n",
    "\n",
    "# compute the sum of squared errors for a given dataset, list of centroids \n",
    "# and list of which centroid each datapoint is assigned to\n",
    "def compute_sse(data, centroids, assigned_centroids):\n",
    "    # compute SSE\n",
    "    sse = np.square(np.add(data, -centroids[assigned_centroids])).sum() / len(data)\n",
    "    return sse\n",
    "\n",
    "\n",
    "num_iters = 10\n",
    "\n",
    "## write k-means loop\n",
    "\n",
    "## plot the results (colour each cluster differently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - t-SNE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both PCA and k-means have proven to be excellent techniques for dimension reduction and clustering, their simplicity, while allowing for interpretability, can also be limiting. Since PCA is a strictly linear technique, higher order correlations and complex structure in data can not always be captured by PCA. One technique which does allow for more complex structures in the data to be used, is t-distributed stochastic neighbour embedding, or t-SNE for short. This technique is a nonlinear dimensionality reduction technique typically used for embedding high-dimensional data for visualisation in low-dimensional spaces (2 or 3 dimensions).\n",
    "\n",
    "In short, the t-SNE algorithm works by assigning a Gaussian probability distribution over the high-dimensional space based on the Euclidian distance between samples, where similar samples are assigned a higher probability, and dissimilar objects a lower probability. Likewise, a probability distribution is assigned over the low-dimensional space (which we're trying to find). The algorithm then minimises the Kullback-Leibler divergence (KL divergence: a measure for how close two probability distributions are) between the two distributions with respect to the locations of the lower-dimensional points. Other metrics than the Euclidian distance can be used in the probability distributions, and may be more appropriate depending on the problem.\n",
    "\n",
    "In more detail, if we have a set of $n$ high-dimensional samples $x_1, ..., x_n$, t-SNE first computes the probabilities $p_{ij}$ that are proportional to the similarity of the samples $x_i$ and $x_j$:\n",
    "\\begin{equation}\n",
    "    p_{j | i} = \\frac{\\exp{(-||x_i - x_j||^2 / 2 \\sigma_i^2)}}{\\sum_{k \\neq i} \\exp{(-||x_i - x_k||^2 / 2 \\sigma_i^2)}},\n",
    "\\end{equation}\n",
    "where we explicitely set $p_{i | i} = 0$. $\\sigma_i$ is the bandwith of the Gaussian kernel, and is adapted to the density of the data. The more dense the data, the smaller $\\sigma_i$. Note that $\\sum_j p_{j | i} = 1$ for all $i$. This conditional probability can be understood as the probability that $x_i$ would pick $x_j$ as its neighbour if neighbours were picked in proportion to their probability density under a Gaussian centered at $x_i$.\n",
    "\n",
    "We can now define the probability\n",
    "\\begin{equation}\n",
    "    p_{ij} = \\frac{p_{j | i} + p_{i | j}}{2 n},\n",
    "\\end{equation}\n",
    "note that this probability is symmetric, $p_{i i} = 0$, and $\\sum_{i, j} p_{i j} = 1$. This is then the probability function over the high-dimensional space.\n",
    "\n",
    "The aim of t-SNE is to learn a lower $d$-dimensional representation $y_1, ..., y_n$, with $d$ typically 2 or 3, that reflects $p_{ij}$ as well as possible, i.e. it tries to keep samples similar in the high-dimensional space also close in the low-dimensional space. For this, t-SNE defines another probability distribution over the low-dimensional space:\n",
    "\\begin{equation}\n",
    "    q_{i j} = \\frac{(1 + ||y_i - y_j||^2)^{-1}}{\\sum_k \\sum_{l \\neq k} (1 + ||y_k - y_l||^2)^{-1}},\n",
    "\\end{equation}\n",
    "and again we set $q_{ii} = 0$. \n",
    "\n",
    "The locations of $y_i$ are then determined by minimising the KL divergence of the distribution $P$ from the distribution $Q$:\n",
    "\\begin{equation}\n",
    "    \\mathrm{KL}(P || Q) = \\sum_{i \\neq j} p_{ij} \\log{\\frac{p_{ij}}{q_{ij}}}.\n",
    "\\end{equation}\n",
    "The KL divergence is minimised using gradient descent. \n",
    "\n",
    "Implementation from scratch is a bit too involved for this course, so instead we will use a readily available implemenetation from the Scikit package. Apply t-SNE to the Ising data and play with the available parameters. What do you see?\n",
    "Note: running TSNE might take a while, you can reduce the amount of samples to speed it up (be sure to include samples from all temperatures!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose = 1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(configs_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "color = plt.cm.coolwarm(np.linspace(0.,1.,len(T)))\n",
    "for t in range(len(T)):\n",
    "#     plt.scatter(tsne_results[:, 0], tsne_results[:, 1])\n",
    "    x = tsne_results[1024*t:1024*(t+1), 0]\n",
    "    y = tsne_results[1024*t:1024*(t+1), 1]\n",
    "    plt.plot(x, y, '.', color=color[t])\n",
    "plt.xlabel('TSNE 1')\n",
    "plt.ylabel('TSNE 2')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
