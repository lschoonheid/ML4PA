{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning for Physics and Astronomy**\n",
    "\n",
    "BSc program in Physics and Astronomy (Honours track), University van Amsterdam and Vrije Universiteit Amsterdam.\n",
    "\n",
    "**Academic year 2021-2022**. \n",
    "\n",
    "Course instructors:\n",
    "\n",
    "* Dr. Juan Rojo ([j.rojo@vu.nl](mailto:j.rojo@vu.nl))\n",
    "\n",
    "* Dr. Tommaso Giani ([tgiani@nikhef.nl](mailto:tgiani@nikhef.nl))\n",
    "\n",
    "* Mr. Ryan van Mastrigt ([r.vanmastrigt@uva.nl](mailto:r.vanmastrigt@uva.nl))\n",
    "    \n",
    "Lecture notes, tutorial notebooks, and other learning materials of the course can be found in its [GitHub repository](https://github.com/LHCfitNikhef/ML4PA)\n",
    "\n",
    "## Tutorial 3: Non-linear regression with neural networks\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "https://github.com/rabah-khalek/TF_tutorials\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "This notebook will serve as an introduction to non-linear regression using neural networks. We will learn how to build and train neural networks with TensorFlow, a powerful machine learning library. \n",
    "\n",
    "## Overview\n",
    "\n",
    "In this tutorial, we will be working with the [Gluon dataset](https://github.com/rabah-khalek/TF_tutorials/tree/master/PseudoData). A gluon is an elementary particle which gives rise to the strong force, which is the force responsible for binding together the protons and neutrons together in atomic nuclei.\n",
    "\n",
    "The gluon is exchanged by other elementary particles in the proton and neutron called quarks. These quarks and gluons are more generally referred to as \"partons\".\n",
    "\n",
    "Because of certain physical properties governing the partons, they cannot be observed by expermiments as free particles, and their distribution within the proton and neutron cannot be calculated analytically. Instead, their distribution must be inferred from data collected by high energy particle colliders like the Large Hadron Collider (LHC) at CERN.\n",
    "\n",
    "A fast-moving proton can be described by the parton distribution function (PDF) $f(x)$, which gives the probability density of finding a parton of type $f$ (a specific quark type or gluon) carrying a fraction $x$ of the proton momentum. \n",
    "\n",
    "The pseudodata used in this tutorial is generated from a gluon distribution function $g(x)$, which was determined in an analysis by the [NNPDF collaboration](http://nnpdf.mi.infn.it), aiming to extract the structure of the proton using contemporary machine learning methods. More specifically, NNPDF determines PDFs using neural networks as a minimally biased modeling tool, trained using Genetic Algorithms (and more recently stochastic gradient descent). \n",
    "\n",
    "We will consider this pseudodata to be the *truth* that we're trying to *discover*. What we will actually fit is the *smeared truth*, where Gaussian noise is added to simulate more realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Gluon data set with Pandas\n",
    "\n",
    "There are four separate datasets each containing a total of 1000 gluon PDF predictions computed between $x=[x_{min},1]$; where:\n",
    "- $x_{min} = 10^{-3}$ for `filename1`\n",
    "- $x_{min} = 10^{-4}$ for `filename2`\n",
    "- $x_{min} = 10^{-5}$ for `filename3`\n",
    "- $x_{min} = 10^{-6}$ for `filename4`\n",
    "  \n",
    "Below we use Pandas to import all datapoints from text files. We then use `sklearn` to split the data randomly into 800 points for training and 200 points for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data parsing is done!\n"
     ]
    }
   ],
   "source": [
    "# Import relevant python modules\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The datasets needed are computed by the `ComputeGluon.py` script in PseudoData\n",
    "filename1='./tutorial3_pseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-3.dat' \n",
    "filename2='./tutorial3_pseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-4.dat' \n",
    "filename3='./tutorial3_pseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-5.dat' \n",
    "filename4='./tutorial3_pseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-6.dat' \n",
    "\n",
    "# Headers to skip\n",
    "lines_to_skip = 5\n",
    "\n",
    "# Defining the columns (cv = central value, sd = standard deviation)\n",
    "columns=[\"x\", \"gluon_cv\", \"gluon_sd\"]\n",
    "\n",
    "# Loading data from txt file\n",
    "# Change filename1 to another filename for data that extends to lower x \n",
    "# (see exercises at the bottom of this notebook)\n",
    "df = pd.read_csv(filename1, \n",
    "                 sep=\"\\s+\", \n",
    "                 skiprows=lines_to_skip, \n",
    "                 usecols=[0,1,2], \n",
    "                 names=columns)\n",
    "\n",
    "# Splitting data randomly to train and test using the sklearn library\n",
    "df_train, df_test = train_test_split(df,test_size=0.2,random_state=42)\n",
    "\n",
    "# Sort the split data according to their x values\n",
    "df_train = df_train.sort_values(\"x\")\n",
    "df_test = df_test.sort_values(\"x\")\n",
    "\n",
    "print(\"Data parsing is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why do we scale the input of a Neural Network?\n",
    "\n",
    "Given the use of small weights in the model and the use of error between predictions and expected values, the scale of inputs and outputs used to train the model are an important factor. Unscaled input variables can result in a slow or unstable learning process, whereas unscaled target variables on regression problems can result in exploding gradients causing the learning process to fail.\n",
    "\n",
    "The input variables are those that the network takes on the input or visible layer in order to make a prediction. In this case we simply have one input variable: x.\n",
    "\n",
    "A good rule of thumb is that input variables should be small values, probably in the range of 0 to 1 or standardized with a zero mean and a standard deviation of one.\n",
    "\n",
    "Whether input variables require scaling depends on the specifics of your problem and of each variable. In principle, since the x values we consider here already range from 0 to 1, we most likely do not need to scale the neural network input. For learning purposes, we will do it anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant python modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Shaping the x values for the scaler\n",
    "train_inputs = df_train['x'].to_numpy().reshape((-1,1))\n",
    "test_inputs = df_test['x'].to_numpy().reshape((-1,1))\n",
    "\n",
    "#Scaling input features to help the minimizer\n",
    "train_scaler = StandardScaler()\n",
    "train_scaler.fit(train_inputs)\n",
    "test_scaler = StandardScaler()\n",
    "test_scaler.fit(test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network using TensorFlow##\n",
    "\n",
    "The idea here is to fit the gluon distribution (which cannot be calculated analytically) using a neural network.\n",
    "\n",
    "We will build the neural network of 1 input, 1 hidden layer, and 1 output in two ways:\n",
    "- By using the high-level Keras API (TF_Model)\n",
    "- By constructing a custom NN (USER_Model)\n",
    "\n",
    "We consider a single layered neural network to be sufficient for our exercise here, but feel free to experiment with *deeper* architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "##################################################################\n",
    "# Building NN from the Keras API (layers.Dense)\n",
    "##################################################################\n",
    "\n",
    "class TF_Model(keras.Model):\n",
    "    \n",
    "    def __init__(self,n_features,n_neurons,n_outputs,name=None):\n",
    "        \n",
    "        super(TF_Model, self).__init__(name=name)\n",
    "        tf.random.set_seed(42) # Set seed for parameter initialization\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        self.hidden1 = layers.Dense(n_neurons, activation='sigmoid',input_shape=[n_features])\n",
    "        # Output layer\n",
    "        self.output1 = layers.Dense(n_outputs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Evaluate hidden layer\n",
    "        x = self.hidden1(x)\n",
    "        # Evaluate and return output\n",
    "        return self.output1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lschoonheid/Documents/ML4PA/Personal work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m##################################################################\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Building a custom NN layer class\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# self.w = weights\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# self.b = biases\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m##################################################################\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mNN_Layer\u001b[39;00m(keras\u001b[39m.\u001b[39mModel):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_inputs, n_outputs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lschoonheid/Documents/ML4PA/Personal%20work/tutorial3/tutorial3_NonLinearRegression-NNs.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Building a custom NN layer class\n",
    "# self.w = weights\n",
    "# self.b = biases\n",
    "##################################################################\n",
    "\n",
    "class NN_Layer(keras.Model):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        \n",
    "        super().__init__()\n",
    "        tf.random.set_seed(42) # Set seed for parameter initialization\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        # A truncated normal is used to prevent large starting weights, slowing down training\n",
    "        self.w = tf.Variable(tf.random.truncated_normal([n_inputs, n_outputs]), name='w')\n",
    "        self.b = tf.Variable(tf.zeros([n_outputs]), name='b')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "    \n",
    "        # Compute and return the output of NN layer\n",
    "        y = tf.matmul(x, self.w) + self.b\n",
    "        \n",
    "        return y\n",
    "\n",
    "##################################################################\n",
    "# Defining our model from custom NN layers\n",
    "##################################################################\n",
    "class USER_Model(keras.Model):\n",
    "    \n",
    "    def __init__(self,n_features,n_hidden,n_outputs, name=None):\n",
    "        super(USER_Model, self).__init__(name=name)\n",
    "        \n",
    "        # Build hidden layer\n",
    "        self.hidden1 = NN_Layer(n_features,n_hidden)\n",
    "        # Build output layer\n",
    "        self.output1 = NN_Layer(n_hidden,n_outputs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        # Evaluate hidden layer\n",
    "        x = self.hidden1(x)\n",
    "        # Activate hidden layer\n",
    "        x = tf.nn.sigmoid(x)\n",
    "        # Return output\n",
    "        return self.output1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data modeling and fitting\n",
    "\n",
    "source: https://www-cdf.fnal.gov/physics/statistics/recommendations/modeling.html\n",
    "\n",
    "The next few cells contain the bulk of the minimization routine.\n",
    "\n",
    "Before we dive into it, let's review some important concepts applied:\n",
    "\n",
    "### Chi-square\n",
    "\n",
    "One of the most frequently occurring problems in high-energy physics is to compare an observed distribution with a prediction, for example from a simulation. Indeed, an analysis might be designed to extract some physical parameter from the simulation or prediction which best fits the data.  \n",
    "\n",
    "For data points with Gaussian errors, the likelihood of a gaussian distribution is:\n",
    "\n",
    "$$ L = \\prod_i \\frac{1}{\\sqrt(2\\pi\\sigma^2)} e^{-\\frac{(m_i - d_i)^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "hence the following [MLE](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (maximum (log) likelihood estimation) gives:\n",
    "\n",
    "$$ \\chi^2 = \\sum_i^N \\left(\\frac{m_i - d_i}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "where $m_i$ is the model evaluated at point $x_i$, $d_i$ is the data, and $\\sigma_i$ is the uncertainty on the data at this point.\n",
    "\n",
    "The chi-square expresses the deviation of the observed data from the fit, weighted inversely by the uncertainties in the individual points. The chi-square can be either used to test how well a particular model describes the data or, if the prediction is a function of some parameters then the optimal values of the parameters can be found by minimizing the chi-square.\n",
    "\n",
    "The main pitfall here is that the purely Gaussian case is in fact rather rare, usually because the data points come from Poisson-distributed numbers of events which are not well approximated by Gaussian distributions. Using a standard chi-square approach in such cases leads to biased estimates of both the parameters and their uncertainties.\n",
    "\n",
    "Nevertheless we will assume here that our datapoints' errors are Gaussian, and we will use the chi-square as the loss function in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Number of training instances\n",
    "N_train = len(df_train[\"x\"])\n",
    "# Number of test instances\n",
    "N_test = len(df_test[\"x\"])\n",
    "\n",
    "# Scale training inputs\n",
    "train_x = np.array(train_scaler.transform(train_inputs)).reshape(N_train,1)\n",
    "# Convert training inputs to tensor\n",
    "train_x = tf.convert_to_tensor(train_x,dtype='float32')\n",
    "\n",
    "# Gather data labels (i.e. measurements)\n",
    "train_y = np.array(df_train[\"gluon_cv\"]).reshape(N_train,1)\n",
    "# Gather data uncertainties \n",
    "train_sigma = np.array(df_train[\"gluon_sd\"]).reshape(N_train,1)\n",
    "# Add noise to data\n",
    "np.random.seed(42)\n",
    "train_y += np.random.normal(0, train_sigma)\n",
    "# Convert data and uncertainties to tensors\n",
    "train_y = tf.convert_to_tensor(train_y,dtype='float32')\n",
    "train_sigma = tf.convert_to_tensor(train_sigma,dtype='float32')\n",
    "\n",
    "# Do the same procedure as above with test set\n",
    "test_x = np.array(test_scaler.transform(test_inputs)).reshape(N_test,1)\n",
    "test_y = np.array(df_test[\"gluon_cv\"]).reshape(N_test,1)\n",
    "test_sigma = np.array(df_test[\"gluon_sd\"]).reshape(N_test,1)\n",
    "test_y += np.random.normal(0, test_sigma)\n",
    "test_x = tf.convert_to_tensor(test_x,dtype='float32')\n",
    "test_y = tf.convert_to_tensor(test_y,dtype='float32')\n",
    "test_sigma = tf.convert_to_tensor(test_sigma,dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Number of neurons in input layer (= number of features = 1, which is momentum fraction x)\n",
    "N_features = 1\n",
    "# Number of neurons in hidden layer\n",
    "N_hidden = 20\n",
    "# Number of neurons in output layer\n",
    "N_output = 1\n",
    "\n",
    "# Initialize model\n",
    "model = TF_Model(N_features,N_hidden,N_output,name='my_TF_model')\n",
    "#model = USER_Model(N_features,N_hidden,N_output,name='my_custom_model')\n",
    "\n",
    "# User-defined loss function (chi-squared)\n",
    "def loss_function(predictions,labels,label_errors):\n",
    "    return tf.reduce_sum(tf.math.square((labels-predictions)/label_errors))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Define tensorflow function for training step (computing and applying gradients)\n",
    "@tf.function\n",
    "def train_step(inputs, labels, label_errors):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(predictions,labels,label_errors)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "# Define tensorflow function for computing loss\n",
    "@tf.function\n",
    "def compute_loss(inputs, labels, label_errors):\n",
    "    \n",
    "    predictions = model(inputs)\n",
    "    loss = loss_function(predictions,labels,label_errors)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Number of training epochs\n",
    "training_epochs = 20000\n",
    "# Epoch intervals to print\n",
    "display = 1000\n",
    "\n",
    "for epoch in range(training_epochs+1):\n",
    "    \n",
    "    train_loss = train_step(train_x, train_y, train_sigma)\n",
    "    test_loss = compute_loss(test_x, test_y, test_sigma)\n",
    "    \n",
    "    template = 'Epoch {:d}, Chi2/Npts: {:.2f}, Test Chi2/Npts: {:.2f}'\n",
    "    if epoch%display==0:\n",
    "        print(template.format(epoch, train_loss.numpy()/N_train, test_loss.numpy()/N_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting output\n",
    "\n",
    "Here's a simple plotting script where:\n",
    "- $t_{g}^{train}$ represents the truth value of the training set, i.e the values taken from the txt file.\n",
    "- $d_{g}^{train}$ are the smeared data we actually fit.\n",
    "- $NN_{g}$ is the output of our Neural Network.\n",
    "\n",
    "You can uncomment several other information like: $d_{g}^{test}$, the error bars used, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "rc('font',**{'family':'serif','serif':['Times']})\n",
    "# rc('text', usetex=True)\n",
    "\n",
    "# Compute predictions and chi2 with latest model parameters\n",
    "\n",
    "prediction_values = model(train_x)\n",
    "chi2_per_Ndata = loss_function(prediction_values,train_y,train_sigma).numpy()/N_train\n",
    "\n",
    "# Plot results\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(111)\n",
    "ax.plot(df_train[\"x\"],df_train[\"gluon_cv\"],color='darkblue',label='$t_{g}^{train}$', lw=3)\n",
    "#ax.plot(df_test[\"x\"],df_test[\"gluon_cv\"],color='darkgreen',label='$t_{g}^{test}$',alpha=0.3)\n",
    "\n",
    "#ax.scatter(df_train[\"x\"],train_y.numpy(), marker=\"o\", color='darkblue',alpha=1.,label='$d_{g}^{train}$')\n",
    "#ax.scatter(df_test[\"x\"],test_y.numpy(),marker=\"o\",  color = 'darkgreen',alpha=1.,label='$d_{g}^{test}$')\n",
    "\n",
    "ax.errorbar(df_train[\"x\"],train_y.numpy(),yerr=train_sigma.numpy(),fmt='.',\n",
    "            color='darkblue',label='$d_{g}^{train}$',alpha=0.3)\n",
    "#ax.errorbar(df_test[\"x\"],test_y.numpy(),yerr=test_sigma.numpy(),fmt='.',\n",
    "#            color='darkgreen',label='$d_{g}^{test}$',alpha=0.5)\n",
    "\n",
    "ax.plot(df_train[\"x\"],prediction_values.numpy(),color='red',label='$NN_{g}$', lw=3)\n",
    "ax.fill_between(df_train[\"x\"],df_train[\"gluon_cv\"]+df_train[\"gluon_sd\"],\n",
    "                df_train[\"gluon_cv\"]-df_train[\"gluon_sd\"] ,color='blue', \n",
    "                alpha=0.1, label='$\\sigma_{g}$')\n",
    "\n",
    "#df_train.plot(kind='line',x='x',y=['gluon_cv'], yerr='gluon_sd',color=['red'], ax=ax)\n",
    "\n",
    "ax.text(0.1,0.1,r'$\\chi^2_{\\rm training}/N_{\\rm data} = %.2f$'%chi2_per_Ndata,fontsize=30,transform=ax.transAxes)\n",
    "\n",
    "# Plot settings\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(r'$g(x)$',fontsize=45)\n",
    "ax.set_xlabel(r'$x$',fontsize=45)\n",
    "ax.legend(loc='best',fontsize=30,frameon=False)\n",
    "ax.tick_params(which='both',direction='in',labelsize=30)\n",
    "ax.tick_params(which='major',length=10)\n",
    "ax.tick_params(which='minor',length=5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Note: you must always execute the Prepare Model cell before training the neural network (this resets the neural network parameters).\n",
    "\n",
    "In performing some of the exercises below, it may be helpful to consult the TensorFlow API documentation here: https://www.tensorflow.org/api_docs/python/tf\n",
    "\n",
    "Try modifying (one at a time) the:\n",
    "- Amount of **training epochs**. \n",
    "#### Question 2a.1) How does the model perform on the test set with respect to the amount of training epochs?\n",
    "\n",
    "- **Activation functions**: relu, sigmoid, tanh (go back to the construction of the NN above). \n",
    "#### Question 2a.2) Which activation gives the smallest test chi-squared value for a fixed number of training epochs? \n",
    "\n",
    "- **Architectures**: Increase the number of neurons in the hidden layer. \n",
    "#### Question 2a.3) Do you notice any change in the flexibility of the neural network? Try adding another hidden layer. Do you get better model performance on the test set? Can you think of a reason why this would be the case?\n",
    "- **Learning rates** of the optimizer. \n",
    "#### Question 2a.4) How does the convergence speed change?\n",
    "- **Optimizers**, e.g. `RMSprop` or `Adagrad` (see other options here: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers). \n",
    "#### Question 2a.5) Which optimizer performs the best given the same number of training epochs?\n",
    "\n",
    "Tune the various options listed above. \n",
    "\n",
    "#### Question 2a.6) What is the best test chi-squared that you obtain?\n",
    "\n",
    "Repeat the modifications using a training set that extends to lower x values by changing the filename in first cell. \n",
    "\n",
    "#### Question 2a.7) What happens in the low-x region?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Exercises\n",
    "\n",
    "### Implementing Glorot Initialization\n",
    "\n",
    "It was discovered by Xavier Glorot and Yoshua Bengio (http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) that modifying the normal distribution associated with the initialization of the neural network weights improved training and alleviated potential issues with vanishing or exploding gradients.\n",
    "\n",
    "This so-called Glorot initialization is now widely used in neural network training. \n",
    "\n",
    "Implement Glorot initialization by changing the standard deviation of the truncated normal distribution (in custom built NN) to:\n",
    "\n",
    "$$\\sigma = \\sqrt{\\frac{2}{(n_{inputs}+n_{outputs})}}$$\n",
    "\n",
    "Do you get better performance on the test set given all settings (training epochs, etc.) remain unchanged? \n",
    "\n",
    "### Implementing Cross-validation\n",
    "\n",
    "Often with very flexible parameterizations such as neural networks, it is easy to overfit the training data (i.e. the model starts to capture noisy data behavior). One way to prevent this is by implementing early stopping with cross-validation. This is done by the following steps:\n",
    "\n",
    "1. Create a validation data set by splitting the training set (start with 50/50 partition)\n",
    "2. While training the neural network on the (now reduced) training set, print also the chi-squared for the validation data, which is unseen by the optimizer (much like the test set). \n",
    "3. Implement a break in the training when the validation chi-squared starts to increase (this is known as \"early stopping\")\n",
    "\n",
    "Overfitting is present when the chi-squared for the validation starts to increase while the training set continues to decrease. If you do not see overfitting occuring, try increasing the number training epochs and/or putting more data in the validation set.\n",
    "\n",
    "How does your model now perform on the test set with cross-validation implemented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Select an Interpreter to start Jupyter\n",
      "\u001b[1;31mRun the following command to install 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc3cfd5d04d5bc54c59acd199299ee38f682039e286a66377da9049f63e82a6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
